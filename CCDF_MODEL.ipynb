{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blueblurr/mbalanced-Classification-Analysis-and-Oversampling-Techniques/blob/main/CCDF_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pueIQvrPWvvz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EujLHOCzW9bN"
      },
      "source": [
        "# Section 1: Problem Nature\n",
        "\n",
        "### Problem Context\n",
        "The increasing adoption of credit cards has led to a parallel rise in credit card fraud. Detecting fraudulent transactions is a high-stakes classification problem where failure can result in financial loss, regulatory risk, and reputational damage.\n",
        "\n",
        "### Data Constraints\n",
        "Due to privacy regulations, banks and financial institutions typically operate on siloed datasets. This fragmentation reduces the data available per model, limiting the ability to learn generalisable fraud patterns.\n",
        "\n",
        "In our dataset (sourced from Kaggle), anonymised features have been transformed via Principal Component Analysis (PCA), and raw transaction metadata is unavailable. This restricts interpretability and may complicate strategies like synthetic oversampling.\n",
        "\n",
        "### Class Imbalance Challenge\n",
        "Fraudulent transactions make up a tiny fraction of total data — often <1%. Standard classifiers trained on such data tend to predict the majority class (legitimate transactions) to maximise accuracy, failing to detect fraud.\n",
        "\n",
        "#### Three Strategic Response Options:\n",
        "1. **Data-Level Approach:** Use oversampling (e.g. SMOTE) or undersampling to balance classes.\n",
        "2. **Model-Level Approach:** Adjust model’s cost function or class weights to penalise false negatives more heavily.\n",
        "3. **Framework-Level Approach:** Reframe the task as an anomaly detection problem rather than standard classification.\n",
        "\n",
        "### Project Objective\n",
        "We will evaluate the effectiveness of these strategies by implementing and comparing classical ML models on a real-world fraud detection dataset. Key focus areas:\n",
        "\n",
        "- Navigating limitations of PCA-transformed data\n",
        "- Evaluating model performance using metrics suited for imbalanced binary classification (see Section 2)\n",
        "- Optimising hyperparameters for each model via stratified cross-validation\n",
        "- Comparing models using consistent performance benchmarks based on Metrics explained in Section 2.\n",
        "- We will apply an Application plan for choosing which model is \"best\" via Section 4.\n",
        "\n",
        "\n",
        "# Section 2: Metrics of Interest\n",
        "\n",
        "| **Metric**               | **What It Measures**                                     | **When It's Useful**                                      |\n",
        "| ------------------------ | -------------------------------------------------------- | --------------------------------------------------------- |\n",
        "| **Precision**            | Out of predicted frauds, how many were correct?          | When false positives are costly                           |\n",
        "| **Recall (Sensitivity)** | Out of actual frauds, how many were detected?            | When missing frauds is unacceptable                       |\n",
        "| **F1-Score**             | Balance between Precision and Recall                     | When you need a single number on imbalance                |\n",
        "| **ROC-AUC**              | Model’s ability to distinguish classes at all thresholds | General classifier quality (good baseline)                |\n",
        "| **PR-AUC**               | Tradeoff of Precision vs Recall                          | More informative than ROC-AUC when positive class is rare |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "From this we can formalise our metrics mathematically. Consider the following values:\n",
        "\n",
        "Predicted Positive Given Actual Positive  =  True Positive\t(TP)\n",
        "Predicted Negative Given Actual Postive = False Negative (FN)\n",
        "Predicted Positive Given Actual Negative = False Positive (FP)\n",
        "Predicted Negative Given Actual Negative = True Negative (TN)\n",
        "                     \n",
        "\n",
        "Precision defined by the ratio of the predicted positives that were correct can be written as the following:\n",
        "\n",
        "$Precision  = \\frac{TP}{FP+TP}$\n",
        "\n",
        "Recall defined by the ratio of frauds detected our of total frauds:\n",
        "\n",
        "$Recall = \\frac{TP}{TP+FN}$\n",
        "\n",
        "The F1 score is the Harmonic Mean of the Precision and Recall. The harmonic mean for two values is $H(x_1,x_2) = \\frac{2x_1x_2}{x_1+x_2}$\n",
        "\n",
        "$F1 = \\frac{2 * Recall * Precision}{Recall + Precision }$\n",
        "\n",
        "ROC-AUC consists of two parts. First, the ROC Curve which plots the True Positive Rate (TPR = Recall) vs False Positive Rate (FPR):\n",
        "\n",
        "Essentially, each data point has two features –– it's true value and it's predicted value. From the entire dataset we can determine the TP,FN,FP and TN rates. We can use these to determine precision and recall. We can use these measures to determine the F1 score. So far so good.\n",
        "\n",
        "#### Ranking Classifiers with ROC-AUC\n",
        "\n",
        "We can also use these values in our Confusion Matrix ([TP, FP, TN, FN]) to plot the TPR (our recall) $\\frac{TP}{TP+FN}$ vs FPR(% of negatives wrongly predicted as positives) $\\frac{FP}{FP+TN}$ values.\n",
        "\n",
        "Now typically, we can assume our classification model outputs predictions as a probability between 0 and 1, and attatch a threshold value, by which values larger than, say x, are considered a predicted positive. Subdividing the 0-1 interval into 200 parts, we can have 200 different threshold values such that we have a corresponding 200 Confusion matricies that correlate to a single point on our ROC curve, from here we can plot 200 points, join the dots then calculate the area under the curve. The higher the value of the area, the higher the probability that our model ranks a randomly chosen positive instance higher than a randomly chosen negative instance, which is essentailly measureing our performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####\n",
        "While there aren’t many disadvantages of th AUC approach , it’s important to note that under extreme imbalance of data AUC score may be affected. Furthermore, AUC treats all misclassifications equally. In many real-world scenarios, the costs and benefits associated with different types of errors can vary. AUC doesn’t take this into account and might not fully represent the performance in cases where one type of error is more critical than another.\n",
        "\n",
        "PR-AUC\n",
        "\n",
        "PR-AUC plots Precision vs. Recall at varying thresholds. Unlike ROC-AUC, it focuses solely on the performance over the positive class, making it more informative when the positive class (fraud) is rare. This makes PR-AUC especially valuable in fraud detection tasks where class imbalance skews ROC-based metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odeG1esvXpYI"
      },
      "source": [
        "Let's start representing this maths through code.\n",
        "\n",
        "\n",
        "First we will Load and display our dataset we wish to classify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FJMBVjNXAHY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path = \"/content/creditcard.csv\" #google collab specific\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJUuNON0v8Np"
      },
      "source": [
        "After some data cleaning, we will define a baseline model to be the an un-specificfied (w.r.t hyper params) Random Forest Model. We will assign our data, train test split, import our model, define and fit it using training data and attain predictions on our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeaxFdCS45BR"
      },
      "outputs": [],
      "source": [
        "# Standard Data Cleaning.\n",
        "#We wish class labels to be 0 or 1.\n",
        "#If there's any other value but this binary input, we will identify & delete the row.\n",
        "\n",
        "\n",
        "# Identify and remove rows where 'Class' is not 0 or 1\n",
        "initial_row_count = df.shape[0]\n",
        "df = df[df['Class'].isin([0, 1])]\n",
        "rows_removed = initial_row_count - df.shape[0]\n",
        "\n",
        "if rows_removed > 0:\n",
        "    print(f\"Removed {rows_removed} rows where 'Class' was not 0 or 1.\")\n",
        "else:\n",
        "    print(\"No rows found where 'Class' was not 0 or 1.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cTizyO1X6CS"
      },
      "outputs": [],
      "source": [
        "# Assign Regressors and Class Labels.\n",
        "X = df.drop('Class',axis = 1)\n",
        "y = df['Class']\n",
        "# Split the data into a training set and a test set\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\n",
        "# Import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYRJTCdww-hp"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Define and fit model\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf.fit(X_train,y_train)\n",
        "#Attain predictions for y, as y_hat\n",
        "y_pred = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21vTv0V1w20u"
      },
      "source": [
        "Next, we will form our Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBgbjVU5w2ZE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=rf.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExZ2AejZ76Dp"
      },
      "source": [
        "Let's look at glance through our model's metrics to assess it's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGF43yzv75bl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eLT1cFv87dl"
      },
      "source": [
        "It's clear from this output that the model struggles with classifying positive values  relative to negative ones, something we expected due to severe class imbalance. We'd therefore expect the PR-AUC to be lower than the ROC-AUC, as it is more sensitive to the positive class label predictive perfomance. We will visualise this below and in Section 2, explore methods of addressing this problem to improve our base model. From then on we will explore other models, ways to improve them, and via our model performance metrics conduct a comparative analysis. For now let's visualise the ROC and PR curves :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ0qdDeD7-zp"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get predicted probabilities for the test set\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve using y_test and y_pred\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "# Calculate the PR curve using y_test and y_pred\n",
        "precision, recall, _ = metrics.precision_recall_curve(y_test, y_prob)\n",
        "pr_auc = metrics.auc(recall, precision)\n",
        "#Display the PR curve\n",
        "display1 = metrics.PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=pr_auc)\n",
        "display1.plot()\n",
        "# Display the ROC curve\n",
        "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
        "display.plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUdPXveSBWEz"
      },
      "source": [
        "While our metrics suggest strong fraud detection performance, these results are based on a single, highly imbalanced split with only 12 out of 400+ fraud cases. Metrics like ROC-AUC are overly optimistic due to class imbalance. To validate model robustness, we must evaluate performance using techniques like cross-validation with stratified sampling, focusing on the PR-AUC and F1-Score as our key metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAjvoCOR9UM_"
      },
      "outputs": [],
      "source": [
        "#LET'S DO THIS ALL AGAIN TO NAIL IT IN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2a4C0SDC-Yf"
      },
      "outputs": [],
      "source": [
        "#Import Pandas and Load our data, displaying it also.\n",
        "import pandas as pd\n",
        "path = \"/content/creditcard.csv\"\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggi0FnbtC-G9"
      },
      "outputs": [],
      "source": [
        "#Pre-processing stage\n",
        "\n",
        "# Getting rid of rows lacking binary values in the Class column.\n",
        "# Also removing rows with NaN values in the 'Class' column.\n",
        "\n",
        "# Initial Row shape\n",
        "row_shape = df.shape[0]\n",
        "df = df[df['Class'].isin([0, 1])]\n",
        "rows_removed = row_shape - df.shape[0]\n",
        "\n",
        "if rows_removed > 0:\n",
        "  print(f'Removed {rows_removed} many rows containing non binary.')\n",
        "else:\n",
        "  print(['No rows removed.'])\n",
        "\n",
        "#Assign Regressors and Class Labels\n",
        "X = df.drop('Class', axis =1)\n",
        "y = df['Class']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Split data into traning and test data\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb9UKLLOC-EE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Define and fit the model\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIbHIb84C-A9"
      },
      "outputs": [],
      "source": [
        "#Get a predicted value\n",
        "y_pred = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgXp2IbYC95J"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=rf.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vxVli-ZC9xL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nitk0mimC9he"
      },
      "outputs": [],
      "source": [
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "precision, recall, _ = metrics.precision_recall_curve(y_test, y_prob)\n",
        "pr_auc = metrics.auc(recall, precision)\n",
        "display1 = metrics.PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=pr_auc)\n",
        "display1.plot()\n",
        "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfYYmlDMC9TI"
      },
      "outputs": [],
      "source": [
        "#Let's build on this with Cross validation\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "#Import Pandas and Load our data, displaying it also.\n",
        "import pandas as pd\n",
        "path = \"/content/creditcard.csv\"\n",
        "df = pd.read_csv(path)\n",
        "#Pre-processing stage\n",
        "\n",
        "# Getting rid of rows lacking binary values in the Class column.\n",
        "# Also removing rows with NaN values in the 'Class' column.\n",
        "\n",
        "# Initial Row shape\n",
        "row_shape = df.shape[0]\n",
        "df = df[df['Class'].isin([0, 1])]\n",
        "rows_removed = row_shape - df.shape[0]\n",
        "\n",
        "if rows_removed > 0:\n",
        "  print(f'Removed {rows_removed} many rows containing non binary.')\n",
        "else:\n",
        "  print(['No rows removed.'])\n",
        "\n",
        "#Assign Regressors and Class Labels\n",
        "X = df.drop('Class', axis =1)\n",
        "y = df['Class']\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=0, verbose=1)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "pr_auc_scores = cross_val_score(rf, X, y, cv=cv, scoring='average_precision')\n",
        "f1_scores = cross_val_score(rf, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "print(\"Mean PR-AUC:\", pr_auc_scores.mean())\n",
        "print(\"Mean F1 Score:\", f1_scores.mean())\n",
        "\n",
        "#OUTPUT WAS:\n",
        "#Mean PR-AUC: 0.8503585364185218\n",
        "#Mean F1 Score: 0.8656535196070081"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZmaXBA7aYJ6"
      },
      "source": [
        "#Section 2: Resampling Techniques\n",
        "\n",
        "A valid approach in addressing class imbalance is to oversample positive cases and/or undersample negative cases from our dataset.\n",
        "\n",
        "##Synthetic Minority Oversampling Technique\n",
        "\n",
        "SMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.\n",
        "\n",
        "\n",
        "Though this algorithm is quite useful, it has few drawbacks associated with it.\n",
        "\n",
        "The synthetic instances generated are in the same direction i.e. connected by an artificial line its diagonal instances. This in turn complicates the decision surface generated by few classifier algorithms.\n",
        "SMOTE tends to create a large no. of noisy data points in feature space\n",
        "\n",
        "We'll illistrate this via code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwEAarg6yg2u",
        "outputId": "966d8adb-29c4-4b9f-c9a8-aacb757a7321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp9sMPYzi8Uu"
      },
      "source": [
        "###First Preprocessing: Load and Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwHWRIzyitxK",
        "outputId": "498c5ff5-fe39-44a7-df8d-31d4501f34bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (284807, 31)\n",
            "Original class distribution:\n",
            " Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n",
            "After filtering shape: (284807, 31)\n",
            "Filtered class distribution:\n",
            " Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/creditcard.csv\")\n",
        "print(\"Original shape:\", df.shape)\n",
        "print(\"Original class distribution:\\n\", df['Class'].value_counts())\n",
        "\n",
        "# Clean Class column: convert to float then to int (ensures uniformity of data type)\n",
        "df['Class'] = pd.to_numeric(df['Class'], errors='coerce')  # handles strings, NaNs\n",
        "df = df[df['Class'].isin([0, 1])]  # remove non-binary values in class\n",
        "df['Class'] = df['Class'].astype(int)\n",
        "\n",
        "# Drop any remaining NaNs\n",
        "df = df.dropna(subset=['Class'])\n",
        "\n",
        "print(\"After filtering shape:\", df.shape)\n",
        "print(\"Filtered class distribution:\\n\", df['Class'].value_counts())\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop('Class', axis=1).reset_index(drop=True)\n",
        "y = df['Class'].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwW2ITgJjCjX"
      },
      "source": [
        "###Second Preprocessing: SMOTE to oversample Postive instance training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypzQ0m_BjBTf"
      },
      "outputs": [],
      "source": [
        "####### OKAY THIS IS SMOTE ON THE DATASET\n",
        "\n",
        "# #Import SMOTE\n",
        "#\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from collections import Counter\n",
        "#\n",
        "# counter = Counter(y_train)\n",
        "# print('Before', counter)\n",
        "# smt = SMOTE(X_train,y_train,random_state =1)\n",
        "# X_train,y_train = smt.fit_resample(X_train,y_train)\n",
        "# counter = Counter(y_train)\n",
        "# print('After', counter)\n",
        "#\n",
        "# #...go on to run model on this data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ysLS3rR-uoQA"
      },
      "outputs": [],
      "source": [
        "# THIS IS SMOTE AS PART OF THE PIPLINE\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn import pipeline\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import average_precision_score, f1_score\n",
        "\n",
        "smt = SMOTE(random_state=1)\n",
        "pipeline = pipeline.Pipeline([\n",
        "    ('SMOTE', smt),\n",
        "    ('clf', RandomForestClassifier(n_jobs=-1, random_state=0, verbose=1))\n",
        "])\n",
        "\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "pr_auc_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='average_precision')\n",
        "f1_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "print(\"Mean PR-AUC:\", pr_auc_scores.mean())\n",
        "print(\"Mean F1 Score:\", f1_scores.mean())\n",
        "\n",
        "#This ended up giving the following:\n",
        "#Mean PR-AUC: 0.8533183273188725\n",
        "#Mean F1 Score: 0.8612774208614977"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jxJLoDViNqB"
      },
      "source": [
        "#ADASYN: Adaptive Synthetic Sampling Approach\n",
        "ADASYN is a generalized form of the SMOTE algorithm. This algorithm also aims to oversample the minority class by generating synthetic instances for it. But the difference here is it considers the density distribution, ri which decides the no. of synthetic instances generated for samples which difficult to learn. Due to this, it helps in adaptively changing the decision boundaries based on the samples difficult to learn. This is the major difference compared to SMOTE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOOQNJQ2kjf2"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "\n",
        "counter_2 = Counter(y_train)\n",
        "print('Before', counter_2)\n",
        "adasyn = ADASYN(X_train,y_train, random_state=1)\n",
        "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
        "print('After', counter_2)\n",
        "\n",
        "#...go on to run model on this data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZEIBckkT1u6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "75e7e04a-1046-46bf-8c51-a46c15c68eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2263937090.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpr_auc_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'average_precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mf1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mall_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 )\n\u001b[0;32m--> 526\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# THIS IS SMOTE AS PART OF THE PIPLINE\n",
        "\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn import pipeline\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import average_precision_score, f1_score\n",
        "\n",
        "ada = ADASYN(random_state=1)\n",
        "pipeline = pipeline.Pipeline([\n",
        "    ('ADASYN', ada),\n",
        "    ('clf', RandomForestClassifier(n_jobs=-1, random_state=0, verbose=1))])\n",
        "\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "pr_auc_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='average_precision')\n",
        "f1_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "print(\"Mean PR-AUC:\", pr_auc_scores.mean())\n",
        "print(\"Mean F1 Score:\", f1_scores.mean())\n",
        "\n",
        "# Mean PR-AUC: 0.8508376667850875\n",
        "# Mean F1 Score: 0.8573482376796676"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koGqEvGPiNBq"
      },
      "source": [
        "#Hybridization: SMOTE + ENN\n",
        "SMOTE + ENN is another hybrid technique where more no. of observations are removed from the sample space. Here, ENN is an undersampling technique where the nearest neighbors of each of the majority class is estimated. If the nearest neighbors misclassify that particular instance of the majority class, then that instance gets deleted.\n",
        "\n",
        "Integrating this technique with oversampled data done by SMOTE helps in doing extensive data cleaning. Here on misclassification by NN’s samples from both the classes are removed. This results in a more clear and concise class separation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGI9hG3MPKBM"
      },
      "outputs": [],
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "from collections import Counter\n",
        "\n",
        "counter_3 = Counter(y_train)\n",
        "print('Before', counter_3)\n",
        "smote_enn = SMOTEENN(X_train,y_train, random_state=1)\n",
        "X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n",
        "print('After', counter_3)\n",
        "\n",
        "#...go on to run model on this data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkVany06ja7s",
        "outputId": "8bd2edfa-201d-4bee-b0f1-1f25cf2bcaf7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   43.8s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.7min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   40.7s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   42.1s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   42.6s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.1s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.2s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   40.4s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.3s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   42.0s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   40.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean PR-AUC: 0.8514091222163511\n",
            "Mean F1 Score: 0.8490642656735388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
          ]
        }
      ],
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn import pipeline\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import average_precision_score, f1_score\n",
        "\n",
        "smt_en = SMOTEENN(random_state=1)\n",
        "pipeline = pipeline.Pipeline([\n",
        "    ('smt_en', smt_en),\n",
        "    ('clf', RandomForestClassifier(n_jobs=-1, random_state=0, verbose=1))])\n",
        "\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "pr_auc_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='average_precision')\n",
        "f1_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "print(\"Mean PR-AUC:\", pr_auc_scores.mean())\n",
        "print(\"Mean F1 Score:\", f1_scores.mean())\n",
        "\n",
        "#Mean PR-AUC: 0.8514091222163511\n",
        "#Mean F1 Score: 0.8490642656735388"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oversampling-based methods, including SMOTE and SMOTE-ENN, did not materially improve model performance over the baseline Random Forest model.\n",
        "\n",
        "This implies the model generalises just fine without synthetic data generation.\n",
        "\n",
        "We will now conisder some directions forward:\n",
        "\n",
        "-Add class_weight='balanced' to RandomForestClassifier (weighting cost function to prioritise reducing false negatives over false positives)\n",
        "\n",
        "-Try XGBoost / LightGBM with the same CV structure (alternative models)\n",
        "\n",
        "-Afterwards, conduct threshold tuning (optional if performance is already strong)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P-Dj5raaZ2pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WEIGHT CLASS BALANCED\n",
        "\n",
        "#Let's build on this with Cross validation\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=0, verbose=1, class_weight='balanced')\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "pr_auc_scores = cross_val_score(rf, X, y, cv=cv, scoring='average_precision')\n",
        "f1_scores = cross_val_score(rf, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "print(\"Mean PR-AUC:\", pr_auc_scores.mean())\n",
        "print(\"Mean F1 Score:\", f1_scores.mean())\n",
        "\n",
        "#OUTPUT WAS:\n",
        "#Mean PR-AUC: 0.8507911190464504\n",
        "#Mean F1 Score: 0.8507171820397957"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYzZ0tkCaHWi",
        "outputId": "a34e6a29-f6eb-4f60-e308-334cd81dd28d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.0s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   31.6s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.0s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   30.6s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.0s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   31.7s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.1s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   33.3s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.8s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   32.5s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.2s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   31.2s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.0s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   30.7s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.6s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   33.0s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.5s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   34.1s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean PR-AUC: 0.8507911190464504\n",
            "Mean F1 Score: 0.8507171820397957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   32.1s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Class-Weighted Random Forest\n",
        "\n",
        "To address class imbalance without altering the data distribution, we trained a Random Forest classifier using the `class_weight='balanced'` parameter. This forces the model to penalize misclassifications of the minority class more heavily during training.\n",
        "\n",
        "**Cross-validated Results:**\n",
        "- PR-AUC: 0.8508\n",
        "- F1 Score: 0.8507\n",
        "\n",
        "**Conclusion:**  \n",
        "Class weighting did not improve model performance relative to the baseline model. In fact, it slightly reduced F1 score. This confirms our earlier conclusion — that the baseline model already handles the imbalance effectively, and additional reweighting or resampling adds no measurable benefit."
      ],
      "metadata": {
        "id": "ILZxQkD7kYe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "##ALTERNATIVE MODELS\n",
        "\n",
        "XGBoost builds an ensemble of decision trees, where each new tree attempts to correct the errors made by the previous ones. It uses a technique called gradient boosting, which involves optimising a loss function (like mean squared error or log loss) using gradient descent.\n",
        "\n",
        "The model has moderately high performance and extremely fast runtimes.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qiY9osp5kgwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "66_ZlMzwW8Ck",
        "outputId": "73b73211-97d7-45e9-9fab-7b984400e51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.0)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll illustrate this approach via code:"
      ],
      "metadata": {
        "id": "IsszOW6_9-fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "xgb_classifier = XGBClassifier(objective='binary:logistic', random_state=0)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "pr_auc_scores = cross_val_score(xgb_classifier, X, y, cv=cv, scoring='average_precision')\n",
        "f1_scores = cross_val_score(xgb_classifier, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "print(\"Mean PR-AUC:\", pr_auc_scores.mean())\n",
        "print(\"Mean F1 Score:\", f1_scores.mean())\n",
        "\n",
        "#Mean PR-AUC: 0.7904463093843405\n",
        "#Mean F1 Score: 0.8212182283730669\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nclCDBadXX9b",
        "outputId": "1dea761b-fe6b-42e9-f91a-a7a3e0d5c36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean PR-AUC: 0.7904463093843405\n",
            "Mean F1 Score: 0.8212182283730669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost's speed is a major advantage, completing training in under two minutes. To optimize its performance, we apply Bayesian hyperparameter optimization using the Tree-structured Parzen Estimator (TPE) method via the Optuna library. This approach efficiently explores the hyperparameter space without exhaustively retraining the model at every iteration, unlike traditional grid search. Importantly, this method is CPU-efficient, reducing the risk of runtime disconnection."
      ],
      "metadata": {
        "id": "MAswz5b2-MMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qyd87_ltmRBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "import optuna\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/creditcard.csv\"  # adjust if local\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Clean the target column\n",
        "df = df[df['Class'].isin([0, 1])]\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Objective function for Optuna: defines hyperparameter search space\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'scale_pos_weight': Counter(y)[0] / Counter(y)[1],\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "        'tree_method': 'hist',  # Fast training\n",
        "        'use_label_encoder': False\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**params, objective='binary:logistic', random_state=0, n_jobs=-1)\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring='average_precision', n_jobs=-1)\n",
        "\n",
        "    return scores.mean()\n",
        "    # returns cross-validated PR-AUC score for XGBoost classifier"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_oNGOQGDo33t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Searches for model that optimises PR- AUC via TPE approach (maximising expected improvement)\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HTJqkcePsqkk",
        "outputId": "5bd98aa6-1dba-4f26-8b3f-bd949597cba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-05 14:47:09,272] A new study created in memory with name: no-name-30a7ab83-fcff-4c4a-9e51-626d04df0e64\n",
            "[I 2025-08-05 14:47:56,213] Trial 0 finished with value: 0.7408389591792222 and parameters: {'max_depth': 4, 'learning_rate': 0.01230072864513482, 'subsample': 0.919571847160622, 'colsample_bytree': 0.5612943109554946, 'n_estimators': 320, 'min_child_weight': 4, 'gamma': 1.7468914763585826}. Best is trial 0 with value: 0.7408389591792222.\n",
            "[I 2025-08-05 14:48:28,973] Trial 1 finished with value: 0.744453278890908 and parameters: {'max_depth': 6, 'learning_rate': 0.0034480565241761115, 'subsample': 0.9519230227400952, 'colsample_bytree': 0.5761271774033399, 'n_estimators': 184, 'min_child_weight': 9, 'gamma': 1.6821284584839185}. Best is trial 1 with value: 0.744453278890908.\n",
            "[I 2025-08-05 14:49:06,702] Trial 2 finished with value: 0.8499250689923574 and parameters: {'max_depth': 10, 'learning_rate': 0.07480970531729769, 'subsample': 0.9526982139792209, 'colsample_bytree': 0.8129976818894193, 'n_estimators': 162, 'min_child_weight': 8, 'gamma': 3.195714715689126}. Best is trial 2 with value: 0.8499250689923574.\n",
            "[I 2025-08-05 14:50:07,049] Trial 3 finished with value: 0.8539686221945404 and parameters: {'max_depth': 3, 'learning_rate': 0.18091823531021184, 'subsample': 0.547555483104754, 'colsample_bytree': 0.8588751078438288, 'n_estimators': 470, 'min_child_weight': 10, 'gamma': 2.9121192775501514}. Best is trial 3 with value: 0.8539686221945404.\n",
            "[I 2025-08-05 14:50:28,278] Trial 4 finished with value: 0.7548127961011712 and parameters: {'max_depth': 3, 'learning_rate': 0.07250325813077618, 'subsample': 0.8406231077646493, 'colsample_bytree': 0.5428106221511547, 'n_estimators': 129, 'min_child_weight': 9, 'gamma': 1.043241206068285}. Best is trial 3 with value: 0.8539686221945404.\n",
            "[I 2025-08-05 14:50:55,381] Trial 5 finished with value: 0.8537977442876461 and parameters: {'max_depth': 4, 'learning_rate': 0.27449023176845805, 'subsample': 0.6617132016714431, 'colsample_bytree': 0.8387432367864314, 'n_estimators': 144, 'min_child_weight': 8, 'gamma': 3.309058708384025}. Best is trial 3 with value: 0.8539686221945404.\n",
            "[I 2025-08-05 14:51:51,812] Trial 6 finished with value: 0.8586183335586715 and parameters: {'max_depth': 5, 'learning_rate': 0.06605719394725824, 'subsample': 0.9587159590933418, 'colsample_bytree': 0.5916666495002252, 'n_estimators': 441, 'min_child_weight': 1, 'gamma': 3.313890348452605}. Best is trial 6 with value: 0.8586183335586715.\n",
            "[I 2025-08-05 14:53:03,094] Trial 7 finished with value: 0.8608646230998902 and parameters: {'max_depth': 8, 'learning_rate': 0.04446314877330555, 'subsample': 0.9562400325311121, 'colsample_bytree': 0.8594102026675077, 'n_estimators': 457, 'min_child_weight': 6, 'gamma': 1.7481741128840915}. Best is trial 7 with value: 0.8608646230998902.\n",
            "[I 2025-08-05 14:54:10,343] Trial 8 finished with value: 0.743193604664722 and parameters: {'max_depth': 6, 'learning_rate': 0.0028967912266322847, 'subsample': 0.8858281228042953, 'colsample_bytree': 0.7847718593666166, 'n_estimators': 424, 'min_child_weight': 6, 'gamma': 4.452154881227351}. Best is trial 7 with value: 0.8608646230998902.\n",
            "[I 2025-08-05 14:54:44,335] Trial 9 finished with value: 0.7546631420325507 and parameters: {'max_depth': 10, 'learning_rate': 0.005607961936091207, 'subsample': 0.7616498132411835, 'colsample_bytree': 0.6421338138903956, 'n_estimators': 142, 'min_child_weight': 9, 'gamma': 1.1194020491517827}. Best is trial 7 with value: 0.8608646230998902.\n",
            "[I 2025-08-05 14:56:02,285] Trial 10 finished with value: 0.844609286655378 and parameters: {'max_depth': 8, 'learning_rate': 0.021237655312105187, 'subsample': 0.7867295900576519, 'colsample_bytree': 0.991758520531171, 'n_estimators': 354, 'min_child_weight': 4, 'gamma': 0.013520856679933413}. Best is trial 7 with value: 0.8608646230998902.\n",
            "[I 2025-08-05 14:57:12,972] Trial 11 finished with value: 0.855888421070951 and parameters: {'max_depth': 8, 'learning_rate': 0.03407732883674363, 'subsample': 0.9955793580700969, 'colsample_bytree': 0.6847071142406488, 'n_estimators': 484, 'min_child_weight': 1, 'gamma': 4.270302121268209}. Best is trial 7 with value: 0.8608646230998902.\n",
            "[I 2025-08-05 14:58:23,273] Trial 12 finished with value: 0.862335070275688 and parameters: {'max_depth': 8, 'learning_rate': 0.06339387546502068, 'subsample': 0.6484181649965302, 'colsample_bytree': 0.9367833869206439, 'n_estimators': 395, 'min_child_weight': 1, 'gamma': 2.2809311356279993}. Best is trial 12 with value: 0.862335070275688.\n",
            "[I 2025-08-05 14:59:43,646] Trial 13 finished with value: 0.794636040844839 and parameters: {'max_depth': 8, 'learning_rate': 0.010566686189231815, 'subsample': 0.6683153995278918, 'colsample_bytree': 0.9380539787529489, 'n_estimators': 387, 'min_child_weight': 5, 'gamma': 2.1490849300229695}. Best is trial 12 with value: 0.862335070275688.\n",
            "[I 2025-08-05 15:00:40,882] Trial 14 finished with value: 0.8512536631640255 and parameters: {'max_depth': 9, 'learning_rate': 0.0351044790498366, 'subsample': 0.6579543422856451, 'colsample_bytree': 0.9003779625070615, 'n_estimators': 239, 'min_child_weight': 3, 'gamma': 0.3523907706358591}. Best is trial 12 with value: 0.862335070275688.\n",
            "[I 2025-08-05 15:01:30,816] Trial 15 finished with value: 0.8611427930919167 and parameters: {'max_depth': 7, 'learning_rate': 0.12944131576588658, 'subsample': 0.5077361729410477, 'colsample_bytree': 0.7198826942796447, 'n_estimators': 279, 'min_child_weight': 6, 'gamma': 2.513006756609023}. Best is trial 12 with value: 0.862335070275688.\n",
            "[I 2025-08-05 15:02:18,049] Trial 16 finished with value: 0.8586213635577543 and parameters: {'max_depth': 7, 'learning_rate': 0.13969323116255347, 'subsample': 0.5046266313632827, 'colsample_bytree': 0.6998364313468538, 'n_estimators': 263, 'min_child_weight': 2, 'gamma': 2.6318584038421418}. Best is trial 12 with value: 0.862335070275688.\n",
            "[I 2025-08-05 15:03:09,082] Trial 17 finished with value: 0.7439555867433085 and parameters: {'max_depth': 7, 'learning_rate': 0.001237207530419098, 'subsample': 0.5941814970201479, 'colsample_bytree': 0.7443633646838288, 'n_estimators': 285, 'min_child_weight': 7, 'gamma': 3.930218978626866}. Best is trial 12 with value: 0.862335070275688.\n",
            "[I 2025-08-05 15:03:51,513] Trial 18 finished with value: 0.864480585050065 and parameters: {'max_depth': 7, 'learning_rate': 0.13150316985574947, 'subsample': 0.5894977307760564, 'colsample_bytree': 0.7555183761504157, 'n_estimators': 218, 'min_child_weight': 5, 'gamma': 2.2798547246120617}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:04:22,471] Trial 19 finished with value: 0.861717213081658 and parameters: {'max_depth': 9, 'learning_rate': 0.27726137010075824, 'subsample': 0.6045980921520606, 'colsample_bytree': 0.9850439397291002, 'n_estimators': 200, 'min_child_weight': 3, 'gamma': 4.998609446625965}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:05:19,591] Trial 20 finished with value: 0.8332168840787919 and parameters: {'max_depth': 5, 'learning_rate': 0.02131022314703478, 'subsample': 0.7046268019358677, 'colsample_bytree': 0.9189095580907227, 'n_estimators': 343, 'min_child_weight': 2, 'gamma': 0.9570392154539671}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:05:51,885] Trial 21 finished with value: 0.8531187918970943 and parameters: {'max_depth': 9, 'learning_rate': 0.2504873616086416, 'subsample': 0.598354626993609, 'colsample_bytree': 0.9901904614768974, 'n_estimators': 203, 'min_child_weight': 3, 'gamma': 4.958629994370712}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:06:37,726] Trial 22 finished with value: 0.8644343399818807 and parameters: {'max_depth': 9, 'learning_rate': 0.11135541791987813, 'subsample': 0.5969637842482893, 'colsample_bytree': 0.9606590808311802, 'n_estimators': 222, 'min_child_weight': 4, 'gamma': 2.247029129501285}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:07:29,079] Trial 23 finished with value: 0.8604647257272398 and parameters: {'max_depth': 9, 'learning_rate': 0.09968087552623854, 'subsample': 0.5671431023513821, 'colsample_bytree': 0.9470289913565498, 'n_estimators': 245, 'min_child_weight': 5, 'gamma': 2.1255442571917094}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:07:58,835] Trial 24 finished with value: 0.8086215449156544 and parameters: {'max_depth': 10, 'learning_rate': 0.04504848838767036, 'subsample': 0.7118052709798631, 'colsample_bytree': 0.8979651070358939, 'n_estimators': 105, 'min_child_weight': 4, 'gamma': 2.293671634396907}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:09:00,141] Trial 25 finished with value: 0.8639204099046788 and parameters: {'max_depth': 7, 'learning_rate': 0.10907033263354071, 'subsample': 0.6298536625400688, 'colsample_bytree': 0.7852611627330291, 'n_estimators': 401, 'min_child_weight': 2, 'gamma': 1.5184845898408967}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:09:43,278] Trial 26 finished with value: 0.8583664851814514 and parameters: {'max_depth': 6, 'learning_rate': 0.17549500702918314, 'subsample': 0.552612081250015, 'colsample_bytree': 0.7834663690061593, 'n_estimators': 238, 'min_child_weight': 2, 'gamma': 1.5347630159892103}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:10:41,814] Trial 27 finished with value: 0.8631177622525354 and parameters: {'max_depth': 7, 'learning_rate': 0.09211178385879638, 'subsample': 0.6210162927658488, 'colsample_bytree': 0.6603886353701821, 'n_estimators': 313, 'min_child_weight': 5, 'gamma': 1.3781577644855787}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:11:23,875] Trial 28 finished with value: 0.8624968822844643 and parameters: {'max_depth': 6, 'learning_rate': 0.18236376659250933, 'subsample': 0.7048207022984281, 'colsample_bytree': 0.7637771745529627, 'n_estimators': 222, 'min_child_weight': 4, 'gamma': 0.5974899010149288}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:12:21,509] Trial 29 finished with value: 0.7657923873422706 and parameters: {'max_depth': 5, 'learning_rate': 0.012221836716120127, 'subsample': 0.5439501603998592, 'colsample_bytree': 0.8190171785656277, 'n_estimators': 346, 'min_child_weight': 3, 'gamma': 1.884059305955101}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:13:05,797] Trial 30 finished with value: 0.8609346076550688 and parameters: {'max_depth': 7, 'learning_rate': 0.10887149896769951, 'subsample': 0.798878615461285, 'colsample_bytree': 0.5196301917060536, 'n_estimators': 306, 'min_child_weight': 7, 'gamma': 2.8648935404965012}. Best is trial 18 with value: 0.864480585050065.\n",
            "[I 2025-08-05 15:14:08,750] Trial 31 finished with value: 0.8645970407806498 and parameters: {'max_depth': 7, 'learning_rate': 0.07659129871161281, 'subsample': 0.6141165259506359, 'colsample_bytree': 0.6305036573037917, 'n_estimators': 318, 'min_child_weight': 5, 'gamma': 1.3486916501414183}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:15:28,229] Trial 32 finished with value: 0.8645956120194246 and parameters: {'max_depth': 7, 'learning_rate': 0.05074438870905114, 'subsample': 0.6402392374976889, 'colsample_bytree': 0.7397995567017471, 'n_estimators': 403, 'min_child_weight': 5, 'gamma': 1.287004546137462}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:16:03,908] Trial 33 finished with value: 0.8404584276132467 and parameters: {'max_depth': 6, 'learning_rate': 0.048079286255761446, 'subsample': 0.5862678696009438, 'colsample_bytree': 0.5982040065544961, 'n_estimators': 176, 'min_child_weight': 4, 'gamma': 0.7859368156168289}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:17:12,680] Trial 34 finished with value: 0.8471188744365973 and parameters: {'max_depth': 8, 'learning_rate': 0.02791601323615249, 'subsample': 0.528465018461449, 'colsample_bytree': 0.6182955512170226, 'n_estimators': 333, 'min_child_weight': 5, 'gamma': 1.906615545308938}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:18:16,612] Trial 35 finished with value: 0.8604728369994327 and parameters: {'max_depth': 5, 'learning_rate': 0.06263080491574867, 'subsample': 0.6816530097156539, 'colsample_bytree': 0.5570089035512095, 'n_estimators': 379, 'min_child_weight': 7, 'gamma': 2.8640467233551283}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:19:03,605] Trial 36 finished with value: 0.8563019189463942 and parameters: {'max_depth': 6, 'learning_rate': 0.18439393087648417, 'subsample': 0.5732658348668316, 'colsample_bytree': 0.7219106992217394, 'n_estimators': 261, 'min_child_weight': 6, 'gamma': 1.2937462459880409}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:19:38,140] Trial 37 finished with value: 0.8478080982696415 and parameters: {'max_depth': 4, 'learning_rate': 0.08458052486513234, 'subsample': 0.6290998855223809, 'colsample_bytree': 0.6540953721763534, 'n_estimators': 208, 'min_child_weight': 5, 'gamma': 0.5250862567601368}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:20:21,290] Trial 38 finished with value: 0.8546605763774915 and parameters: {'max_depth': 9, 'learning_rate': 0.05438798696313875, 'subsample': 0.7300022451684326, 'colsample_bytree': 0.6143637691731118, 'n_estimators': 178, 'min_child_weight': 4, 'gamma': 3.1018895444722556}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:21:20,380] Trial 39 finished with value: 0.8624141805683742 and parameters: {'max_depth': 10, 'learning_rate': 0.15074132782446872, 'subsample': 0.6346959520006468, 'colsample_bytree': 0.5070276121510868, 'n_estimators': 433, 'min_child_weight': 6, 'gamma': 1.83813654360507}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:22:01,098] Trial 40 finished with value: 0.7409172618178748 and parameters: {'max_depth': 4, 'learning_rate': 0.007507241193142543, 'subsample': 0.6114153432845131, 'colsample_bytree': 0.86810126636188, 'n_estimators': 289, 'min_child_weight': 8, 'gamma': 3.690473894091854}. Best is trial 31 with value: 0.8645970407806498.\n",
            "[I 2025-08-05 15:22:58,923] Trial 41 finished with value: 0.8650857211418908 and parameters: {'max_depth': 7, 'learning_rate': 0.11988558780927784, 'subsample': 0.6893851338836529, 'colsample_bytree': 0.8316318944684563, 'n_estimators': 404, 'min_child_weight': 5, 'gamma': 1.5162391893395282}. Best is trial 41 with value: 0.8650857211418908.\n",
            "[I 2025-08-05 15:23:53,659] Trial 42 finished with value: 0.8628948891404468 and parameters: {'max_depth': 7, 'learning_rate': 0.2287740810751458, 'subsample': 0.6890780974412486, 'colsample_bytree': 0.8391164491995629, 'n_estimators': 463, 'min_child_weight': 5, 'gamma': 1.1456392184847348}. Best is trial 41 with value: 0.8650857211418908.\n",
            "[I 2025-08-05 15:25:01,467] Trial 43 finished with value: 0.8651803985314931 and parameters: {'max_depth': 8, 'learning_rate': 0.08011927645640976, 'subsample': 0.6444313604893382, 'colsample_bytree': 0.812649449268795, 'n_estimators': 372, 'min_child_weight': 4, 'gamma': 1.6326169748600887}. Best is trial 43 with value: 0.8651803985314931.\n",
            "[I 2025-08-05 15:26:09,329] Trial 44 finished with value: 0.8626602673287671 and parameters: {'max_depth': 8, 'learning_rate': 0.07672520055259192, 'subsample': 0.659839187218827, 'colsample_bytree': 0.8080794352546886, 'n_estimators': 363, 'min_child_weight': 6, 'gamma': 1.6444014062610115}. Best is trial 43 with value: 0.8651803985314931.\n",
            "[I 2025-08-05 15:27:31,597] Trial 45 finished with value: 0.8619988963651467 and parameters: {'max_depth': 7, 'learning_rate': 0.033132296534008475, 'subsample': 0.747758645282568, 'colsample_bytree': 0.7387995177735737, 'n_estimators': 409, 'min_child_weight': 5, 'gamma': 1.2672708928673284}. Best is trial 43 with value: 0.8651803985314931.\n",
            "[I 2025-08-05 15:28:41,926] Trial 46 finished with value: 0.8621163895989795 and parameters: {'max_depth': 8, 'learning_rate': 0.0806997109057219, 'subsample': 0.6474511482906089, 'colsample_bytree': 0.6818840179548653, 'n_estimators': 370, 'min_child_weight': 4, 'gamma': 0.9001342368350529}. Best is trial 43 with value: 0.8651803985314931.\n",
            "[I 2025-08-05 15:29:55,649] Trial 47 finished with value: 0.8625106888085938 and parameters: {'max_depth': 6, 'learning_rate': 0.06242577749182571, 'subsample': 0.6830691507372155, 'colsample_bytree': 0.7648936025341284, 'n_estimators': 419, 'min_child_weight': 7, 'gamma': 1.9777909919904297}. Best is trial 43 with value: 0.8651803985314931.\n",
            "[I 2025-08-05 15:31:30,559] Trial 48 finished with value: 0.8545082967440678 and parameters: {'max_depth': 8, 'learning_rate': 0.02362973542058531, 'subsample': 0.7758225564157841, 'colsample_bytree': 0.8342689139587907, 'n_estimators': 453, 'min_child_weight': 6, 'gamma': 1.6138384517144586}. Best is trial 43 with value: 0.8651803985314931.\n",
            "[I 2025-08-05 15:32:37,271] Trial 49 finished with value: 0.8284997573520358 and parameters: {'max_depth': 7, 'learning_rate': 0.014621973760366008, 'subsample': 0.81839797073699, 'colsample_bytree': 0.8837526287066113, 'n_estimators': 325, 'min_child_weight': 5, 'gamma': 0.2458022902972472}. Best is trial 43 with value: 0.8651803985314931.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best PR-AUC Score:\", study.best_value)\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n",
        "\n",
        "# Evaluate the best model on PR-AUC and F1\n",
        "best_model = XGBClassifier(**study.best_params, objective='binary:logistic',\n",
        "                           use_label_encoder=False, random_state=0, n_jobs=-1)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "pr_auc = cross_val_score(best_model, X, y, cv=cv, scoring='average_precision', n_jobs=-1)\n",
        "f1 = cross_val_score(best_model, X, y, cv=cv, scoring='f1', n_jobs=-1)\n",
        "\n",
        "print(\"Cross-Validated PR-AUC:\", pr_auc.mean())\n",
        "print(\"Cross-Validated F1 Score:\", f1.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQL3v5w2r0UD",
        "outputId": "bfd2d973-1356-422e-8a7e-dcf3e9a39804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best PR-AUC Score: 0.8651803985314931\n",
            "Best Hyperparameters: {'max_depth': 8, 'learning_rate': 0.08011927645640976, 'subsample': 0.6444313604893382, 'colsample_bytree': 0.812649449268795, 'n_estimators': 372, 'min_child_weight': 4, 'gamma': 1.6326169748600887}\n",
            "Cross-Validated PR-AUC: 0.8618736239214029\n",
            "Cross-Validated F1 Score: 0.8648313654581627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##🧾 Final Summary and Conclusion\n",
        "In this notebook, we tackled the challenge of credit card fraud detection, a high-stakes and highly imbalanced classification problem. We:\n",
        "\n",
        "**Explored baseline performance using a Random Forest classifier**, identifying the limitations of standard accuracy in imbalanced settings.\n",
        "\n",
        "**Applied resampling techniques (SMOTE, ADASYN, SMOTE-ENN) to balance the dataset**, but observed no material performance gains over the baseline.\n",
        "\n",
        "**Evaluated class weighting**, which similarly failed to outperform the default model.\n",
        "\n",
        "**Introduced XGBoost**, a powerful gradient boosting algorithm, and used **Bayesian optimization (Optuna)** to efficiently tune hyperparameters.\n",
        "\n",
        "Improved performance further with **cross-validation, using PR-AUC and F1-score** as our key metrics.\n",
        "\n",
        "🔍 Key Takeaway:\n",
        "Despite experimenting with several imbalance mitigation techniques, **the baseline Random Forest and tuned XGBoost** models handled the dataset robustly without requiring synthetic sampling. This reinforces the value of well-calibrated ensemble methods, especially when combined with rigorous cross-validation and appropriate evaluation metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "lCEn8Stqnh4O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqMvTbvl5PDp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCp9Li2Fki0hpI7072rJxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}